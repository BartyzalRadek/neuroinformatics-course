{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "\n",
    "from tensorflow.contrib.keras import backend as K\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense, Dropout, Flatten, LSTM, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version = 1.3.0\n",
      "Python version = 3.5.2 (default, Nov 17 2016, 17:05:23) \n",
      "[GCC 5.4.0 20160609]\n",
      "Keras backend = tensorflow\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version =\", tf.__version__)\n",
    "#print(\"TF contrib Keras version =\",keras.__version__) ???\n",
    "print(\"Python version =\",sys.version)\n",
    "print(\"Keras backend =\", keras.backend.backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unreasonable effectiveness of RNN: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "Minimal char-rnn in Numpy: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "Minimal char-rnn in Keras: https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = open(filename, 'r').read() # should be simple plain text file\n",
    "    print(\"Loaded data from\", filename)\n",
    "    \n",
    "    chars = sorted(list(set(data)))\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print(\"Data has {} characters, {} unique.\".format(data_size, vocab_size))\n",
    "\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "    print(\"Turn an array of characters to an array of numbers:\")\n",
    "    data_ix = [char_to_ix[char] for char in data]\n",
    "    print(\"  data[0]={} has been turned to: \\n  data_ix[0]={}\".format(data[0], data_ix[0]))\n",
    "\n",
    "    data_one_hot = np.zeros(shape=(data_size, vocab_size), dtype=float)\n",
    "    for i in range(len(data_ix)):\n",
    "        idx = data_ix[i]\n",
    "        data_one_hot[i,idx] = 1.0\n",
    "    print(\"Turn an array of numbers to an array of one-hot encoded vectors:\")\n",
    "    print(\"  data_ix[0]={} has been turned to: \\n  data_one_hot[0]={}\".format(data_ix[0], data_one_hot[0]))\n",
    "    print(\"Returning data_one_hot, data_size, vocab_size, ix_to_char\")\n",
    "    return data_one_hot, data_size, vocab_size, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_char(vec, ix_to_char):\n",
    "    \"\"\"Returns most probable character represented by the 'one-hot' vector of probabilities.\"\"\"\n",
    "    return ix_to_char[np.argmax(vec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from data/first_names.txt\n",
      "Data has 36122 characters, 27 unique.\n",
      "Turn an array of characters to an array of numbers:\n",
      "  data[0]=J has been turned to: \n",
      "  data_ix[0]=10\n",
      "Turn an array of numbers to an array of one-hot encoded vectors:\n",
      "  data_ix[0]=10 has been turned to: \n",
      "  data_one_hot[0]=[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Returning data_one_hot, data_size, vocab_size, ix_to_char\n"
     ]
    }
   ],
   "source": [
    "data_one_hot, data_size, vocab_size, ix_to_char = load_data('data/first_names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from data/first_names.txt\n",
      "Data has 36122 characters, 27 unique.\n",
      "Turn an array of characters to an array of numbers:\n",
      "  data[0]=J has been turned to: \n",
      "  data_ix[0]=10\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/first_names.txt'\n",
    "data = open(filename, 'r').read() # should be simple plain text file\n",
    "print(\"Loaded data from\", filename)\n",
    "\n",
    "data = data.replace('\\n', ' ') #change '\\n' to ' ' for better readability\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"Data has {} characters, {} unique.\".format(data_size, vocab_size))\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "print(\"Turn an array of characters to an array of numbers:\")\n",
    "data_ix = [char_to_ix[char] for char in data]\n",
    "print(\"  data[0]={} has been turned to: \\n  data_ix[0]={}\".format(data[0], data_ix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting text into sequences of seq_len=20 overlapping after each step_size=3 characters:\n",
      "  Sequences shape = (12034, 20) int64\n",
      "  correct_next_char shape = (12034,) int64\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20 #length of a sequence of characters fed to RNN before asking for a next character while training\n",
    "step_size = 3 #first sequence starts at index 0, next one at index 3 etc...\n",
    "print(\"Cutting text into sequences of seq_len={} overlapping after each step_size={} characters:\".format(seq_len, step_size))\n",
    "\n",
    "sequences = [] #list of sequences\n",
    "correct_next_char = [] #correct next char after each sequence - this will be used as target data to train the RNN\n",
    "\n",
    "for i in range(0, len(data) - seq_len, step_size):\n",
    "    sequences.append(data_ix[i:i+seq_len])\n",
    "    correct_next_char.append(data_ix[i+seq_len])\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "correct_next_char = np.array(correct_next_char)\n",
    "\n",
    "print(\"  Sequences shape =\", sequences.shape, sequences.dtype)\n",
    "print(\"  correct_next_char shape =\", correct_next_char.shape, correct_next_char.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing = transforming sequences and next_char to one-hot encoding:\n",
      "  sequences[0,0]=10 has been turned to: \n",
      "  seq_one_hot[0,0]=[False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False]\n",
      "  correct_next_char[0]=3 has been turned to: \n",
      "  next_char_one_hot[0]=[False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False]\n",
      "  x = seq_one_hot shape = (12034, 20, 27) bool\n",
      "  y = next_char_one_hot shape = (12034, 27) bool\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing = transforming sequences and next_char to one-hot encoding:\")\n",
    "seq_one_hot = np.zeros(shape=(len(sequences), seq_len, vocab_size), dtype=np.bool)\n",
    "next_char_one_hot = np.zeros(shape=(len(sequences), vocab_size), dtype=np.bool)\n",
    "\n",
    "for s in range(len(sequences)):\n",
    "    for c in range(seq_len):\n",
    "        idx = sequences[s,c] #index of c-th char in s-th sequence\n",
    "        seq_one_hot[s,c,idx] = 1\n",
    "        \n",
    "    next_char_idx = correct_next_char[s] #index of next char after the s-th sequence\n",
    "    next_char_one_hot[s,next_char_idx] = 1\n",
    "\n",
    "x = seq_one_hot\n",
    "y = next_char_one_hot\n",
    "print(\"  sequences[0,0]={} has been turned to: \\n  seq_one_hot[0,0]={}\".format(sequences[0,0], seq_one_hot[0,0]))\n",
    "print(\"  correct_next_char[0]={} has been turned to: \\n  next_char_one_hot[0]={}\".format(correct_next_char[0], next_char_one_hot[0]))\n",
    "print(\"  x = seq_one_hot shape =\", seq_one_hot.shape, seq_one_hot.dtype)\n",
    "print(\"  y = next_char_one_hot shape =\", next_char_one_hot.shape, next_char_one_hot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(neurons, seq_len, vocab_size):\n",
    "    print('Building single layer LSTM model with {} neurons...'.format(neurons))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(seq_len, vocab_size)))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=0.01))\n",
    "    \n",
    "    print(\"LSTM layer input shape =\", (seq_len, vocab_size))\n",
    "    print(\"Dense layer with vocab_size={} neurons and 'softmax' activation\".format(vocab_size))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building single layer LSTM model with 50 neurons...\n",
      "LSTM layer input shape = (20, 27)\n",
      "Dense layer with vocab_size=27 neurons and 'softmax' activation\n"
     ]
    }
   ],
   "source": [
    "model = build_model(neurons=50, seq_len=seq_len, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training for 10 epochs with batch size = 128\n",
      "------------------------------\n",
      "Epoch 0\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 8s - loss: 1.7974     \n",
      "----- Generating with seed: \"CIA VAL VADA UNA TRE\"\n",
      "CIA VAL VADA UNA TRESTA JENELENE EENELENE EENELENE EENELENE ------------------------------\n",
      "Epoch 1\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 8s - loss: 1.7232     \n",
      "----- Generating with seed: \"ACALYN ISELA IRENA I\"\n",
      "ACALYN ISELA IRENA IENA AISTA AYANA AANNA AANNA AANNA AANNA ------------------------------\n",
      "Epoch 2\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 8s - loss: 1.6576     \n",
      "----- Generating with seed: \"RON ANTONY ANTIONETT\"\n",
      "RON ANTONY ANTIONETTA ARINE ARINE HARLINA ARLENA ARLENA ARLE------------------------------\n",
      "Epoch 3\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 9s - loss: 1.6005     \n",
      "----- Generating with seed: \" ADELINE YOUNG VICEN\"\n",
      " ADELINE YOUNG VICEN CORISTA CORISA CORISA CORISA CORISA COR------------------------------\n",
      "Epoch 4\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 8s - loss: 1.5374     \n",
      "----- Generating with seed: \" EMILE ELYSE ELOY EL\"\n",
      " EMILE ELYSE ELOY ELENERIA ELENERIA ELENERIE ELENERIA ELENER------------------------------\n",
      "Epoch 5\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 9s - loss: 1.4904     \n",
      "----- Generating with seed: \" MARCO LYNETTE EULA \"\n",
      " MARCO LYNETTE EULA VENNIE VENNA VERLE VERLINE VERLIE VERLIA------------------------------\n",
      "Epoch 6\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 8s - loss: 1.4335     \n",
      "----- Generating with seed: \" TRINH TREENA TREASA\"\n",
      " TRINH TREENA TREASA TENIS ANDA ANDRA ANDA ALESSA ALEESSA AL------------------------------\n",
      "Epoch 7\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 9s - loss: 1.3894     \n",
      "----- Generating with seed: \"IM JORGE GREG GORDON\"\n",
      "IM JORGE GREG GORDON GERI GERRI GELANIE GENNIE GENNETTE GENN------------------------------\n",
      "Epoch 8\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 9s - loss: 1.3540     \n",
      "----- Generating with seed: \"IRA MODESTA MIRTA MI\"\n",
      "IRA MODESTA MIRTA MINTI MINTA MINIS MINTI MINTA MINIS MINTI ------------------------------\n",
      "Epoch 9\n",
      "Epoch 1/1\n",
      "12034/12034 [==============================] - 9s - loss: 1.3076     \n",
      "----- Generating with seed: \"HAROLETTE CHARLETTE \"\n",
      "HAROLETTE CHARLETTE CHARLYN CARI CARLINE CARLINE CARLINE CAR"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "generate_chars = 50\n",
    "\n",
    "print(\"Started training for {} epochs with batch size = {}\".format(epochs, batch_size))\n",
    "for epoch in range(epochs):\n",
    "    print(\"\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Epoch\", epoch)\n",
    "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
    "    \n",
    "    seq_start_index = random.randint(0, len(data) - seq_len - 1)\n",
    "    \n",
    "    generated = \"\"\n",
    "    sentence = data[seq_start_index: seq_start_index + seq_len] #sentence = sequence\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(generate_chars):\n",
    "        x_pred = np.zeros((1, seq_len, vocab_size)) #single sequence will be passed to trained RNN\n",
    "        for t, char in enumerate(sentence): #turn sequence to one-hot\n",
    "            x_pred[0, t, char_to_ix[char]] = 1. #sequence is from data = characters not numbers\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0] #get next_char prediction = vector of probabilities\n",
    "        #next_index = sample(preds, diversity) #TODO implement sample function supporting different diversities\n",
    "        next_char = vector_to_char(preds, ix_to_char)\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_1.3_Python3",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
